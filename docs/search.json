[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "CASA0023 - Remote Sensing Learning Diary",
    "section": "",
    "text": "Welcome\nWelcome to the Remote Sensing Learning Diary! This is where I will update my progress in learning how to analyse and make use of remote sensing data!"
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "This is a book created from markdown and executable code.\nSee Knuth (1984) for additional discussion of literate programming.\n\n1 + 1\n\n[1] 2\n\n\n\n\n\n\nKnuth, Donald E. 1984. “Literate Programming.” Comput. J. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97."
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "2  Summary",
    "section": "",
    "text": "In summary, this book has no content whatsoever.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Knuth, Donald E. 1984. “Literate Programming.” Comput.\nJ. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97."
  },
  {
    "objectID": "01_intro.html",
    "href": "01_intro.html",
    "title": "1  Getting Started with Remote Sensing",
    "section": "",
    "text": "This is a book created from markdown and executable code.\nSee Knuth (1984) for additional discussion of literate programming.\n\n1 + 1\n\n[1] 2\n\n\n\n\n\n\nKnuth, Donald E. 1984. “Literate Programming.” Comput. J. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97."
  },
  {
    "objectID": "02_portfolio.html",
    "href": "02_portfolio.html",
    "title": "2  Summary",
    "section": "",
    "text": "In summary, this book has no content whatsoever.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "99_references.html",
    "href": "99_references.html",
    "title": "References",
    "section": "",
    "text": "Amani, Meisam, Arsalan Ghorbanian, Seyed Ali Ahmadi, Mohammad Kakooei,\nArmin Moghimi, S. Mohammad Mirmazloumi, Sayyed Hamed Alizadeh Moghaddam,\net al. 2020. “Google Earth Engine Cloud Computing\nPlatform for Remote Sensing Big Data Applications:\nA Comprehensive Review.” IEEE Journal of\nSelected Topics in Applied Earth Observations and Remote Sensing\n13: 5326–50. https://doi.org/10.1109/JSTARS.2020.3021052.\n\n\nBhatia, S. Y., G. R. Patil, and K. M. Buddhiraju. 2023. “Analysing\nUrban Sprawl of the Mumbai Metropolitan Region Using\nRemote Sensing and Socioeconomic Data.” In\nThe International Archives of\nPhotogrammetry, Remote Sensing and\nSpatial Information Sciences, XLVIII-M-3-2023:35–42.\nGottingen, Germany: Copernicus GmbH. https://doi.org/10.5194/isprs-archives-XLVIII-M-3-2023-35-2023.\n\n\nBuonfantino, Giusy. 2022. “SC Johnson Forecasts\nMosquitoes with Google Earth Engine.” Google\nCloud Blog.\nhttps://cloud.google.com/blog/products/data-analytics/sc-johnson-forecasts-mosquitoes-with-google-earth-engine.\n\n\nEarthLab. 2018. “Learn to Use NAIP Multiband Remote Sensing\nImages in Python.” Earth Data Science -\nEarth Lab.\nhttps://www.earthdatascience.org/courses/use-data-open-source-python/multispectral-remote-sensing/intro-naip/.\n\n\nFoody, Giles M. 2020. “Explaining the Unsuitability of the Kappa\nCoefficient in the Assessment and Comparison of the Accuracy of Thematic\nMaps Obtained by Image Classification.” Remote Sensing of\nEnvironment 239 (March): 111630. https://doi.org/10.1016/j.rse.2019.111630.\n\n\nGeiß, Christian, and Hannes Taubenböck. 2013. “Remote Sensing\nContributing to Assess Earthquake Risk: From a Literature Review Towards\na Roadmap.” Natural Hazards 68 (1): 7–48. https://doi.org/10.1007/s11069-012-0322-2.\n\n\nGhaffarian, Saman, Norman Kerle, and Tatiana Filatova. 2018.\n“Remote Sensing-Based Proxies for Urban\nDisaster Risk Management and Resilience: A\nReview.” Remote Sensing 10 (11): 1760. https://doi.org/10.3390/rs10111760.\n\n\nGillespie, Thomas W., Jasmine Chu, Elizabeth Frankenberg, and Duncan\nThomas. 2007. “Assessment and Prediction of Natural Hazards from\nSatellite Imagery.” Progress in Physical Geography: Earth and\nEnvironment 31 (5): 459–70. https://doi.org/10.1177/0309133307083296.\n\n\nGISGeography. 2017. “The Atmospheric Window in\nRemote Sensing.” GIS Geography.\nhttps://gisgeography.com/atmospheric-window/.\n\n\nGorelick, Noel, Matt Hancher, Mike Dixon, Simon Ilyushchenko, David\nThau, and Rebecca Moore. 2017. “Google Earth Engine:\nPlanetary-scale Geospatial Analysis for\nEveryone.” Remote Sensing of Environment, Big\nRemotely Sensed Data: Tools, applications and experiences,\n202 (December): 18–27. https://doi.org/10.1016/j.rse.2017.06.031.\n\n\nGupta, Kshama, Pramod Kumar, S. K. Pathan, and K. P. Sharma. 2012.\n“Urban Neighborhood Green Index – A\nMeasure of Green Spaces in Urban Areas.” Landscape and Urban\nPlanning 105 (3): 325–35. https://doi.org/10.1016/j.landurbplan.2012.01.003.\n\n\nJensen, John R. 2016. Introductory Digital Image Processing: A\nRemote Sensing Perspective / John R. Jensen\n(University of South Carolina). 4th\nedition. Pearson Series in Geographic Information Science. Glenview, IL:\nPearson Education, Inc.\n\n\nKupidura, Przemysław. 2019. “The Comparison of\nDifferent Methods of Texture Analysis for\nTheir Efficacy for Land Use Classification in\nSatellite Imagery.” Remote Sensing 11 (10):\n1233. https://doi.org/10.3390/rs11101233.\n\n\nKussul, Nataliia, Mykola Lavreniuk, Sergii Skakun, and Andrii Shelestov.\n2017. “Deep Learning Classification of Land\nCover and Crop Types Using Remote Sensing\nData.” IEEE Geoscience and Remote Sensing Letters\n14 (5): 778–82. https://doi.org/10.1109/LGRS.2017.2681128.\n\n\nLiang, Shunlin, and Jindi Wang, eds. 2020. “Chapter 4 -\nAtmospheric Correction of Optical Imagery.” In\nAdvanced Remote Sensing (Second\nEdition), 131–56. Academic Press. https://doi.org/10.1016/B978-0-12-815826-5.00004-0.\n\n\nLovelace, Robin, Jakub Nowosad, and Jannes Muenchow. 2019. Chapter\n12 Statistical Learning \nGeocomputation with R.\n\n\nMapBiomas. n.d. “MapBiomas Alerta.”\nhttps://alerta.mapbiomas.org/en. Accessed February 7, 2024.\n\n\nMing, Dongping, Tianning Zhou, Min Wang, and Tian Tan. 2016. “Land\nCover Classification Using Random Forest with Genetic Algorithm-Based\nParameter Optimization.” Journal of Applied Remote\nSensing 10 (3): 035021. https://doi.org/10.1117/1.JRS.10.035021.\n\n\nNASA. 2019. “What Is Remote Sensing? \nEarthdata.” Backgrounder.\nhttps://www.earthdata.nasa.gov/learn/backgrounders/remote-sensing; Earth\nScience Data Systems, NASA.\n\n\nNowosad, Jakub, and Tomasz Stepinski. 2021. “Generalizing the\nSimple Linear Iterative Clustering (SLIC)\nSuperpixels.” https://nowosad.github.io/giscience-2021/.\n\n\nPaletta, Anthony. 2016. “Story of Cities #32: Jane\nJacobs v Robert Moses, Battle of New\nYork’s Urban Titans.” The Guardian, April.\n\n\nPlaza, A., P. Martinez, R. Perez, and J. Plaza. 2002.\n“Spatial/Spectral Endmember Extraction by Multidimensional\nMorphological Operations.” IEEE Transactions on Geoscience\nand Remote Sensing 40 (9): 2025–41. https://doi.org/10.1109/TGRS.2002.802494.\n\n\nRoberts, David R., Volker Bahn, Simone Ciuti, Mark S. Boyce, Jane Elith,\nGurutzeta Guillera-Arroita, Severin Hauenstein, et al. 2017.\n“Cross-Validation Strategies for Data with Temporal, Spatial,\nHierarchical, or Phylogenetic Structure.” Ecography 40\n(8): 913–29. https://doi.org/10.1111/ecog.02881.\n\n\nRong, Youtong, Ting Zhang, Yanchen Zheng, Chunqi Hu, Ling Peng, and Ping\nFeng. 2020. “Three-Dimensional Urban Flood Inundation Simulation\nBased on Digital Aerial Photogrammetry.” Journal of\nHydrology 584 (May): 124308. https://doi.org/10.1016/j.jhydrol.2019.124308.\n\n\nRozenstein, Offer, and Arnon Karnieli. 2011. “Comparison of\nMethods for Land-Use Classification Incorporating Remote Sensing and\nGIS Inputs.” Applied Geography 31 (2):\n533–44. https://doi.org/10.1016/j.apgeog.2010.11.006.\n\n\nShao, Zhenfeng, Neema S. Sumari, Aleksei Portnov, Fanan Ujoh, Walter\nMusakwa, and Paulo J. Mandela. 2021. “Urban Sprawl and Its Impact\non Sustainable Urban Development: A Combination of Remote Sensing and\nSocial Media Data.” Geo-Spatial Information Science 24\n(2): 241–55. https://doi.org/10.1080/10095020.2020.1787800.\n\n\nSheykhmousa, Mohammadreza, Masoud Mahdianpari, Hamid Ghanbari, Fariba\nMohammadimanesh, Pedram Ghamisi, and Saeid Homayouni. 2020.\n“Support Vector Machine Versus Random Forest for\nRemote Sensing Image Classification: A\nMeta-Analysis and Systematic Review.”\nIEEE Journal of Selected Topics in Applied Earth Observations and\nRemote Sensing 13: 6308–25. https://doi.org/10.1109/JSTARS.2020.3026724.\n\n\nSidhu, Nanki, Edzer Pebesma, and Gilberto Câmara. 2018. “Using\nGoogle Earth Engine to Detect Land Cover Change:\nSingapore as a Use Case.” European Journal of\nRemote Sensing 51 (1): 486–500. https://doi.org/10.1080/22797254.2018.1451782.\n\n\nSkopyk, Brad. 2021. “Georeferencing Historical\nMaps.” ArcGIS StoryMaps.\nhttps://storymaps.arcgis.com/stories/dd75d0398f7d4ded924d303161895b8b.\n\n\nTamiminia, Haifa, Bahram Salehi, Masoud Mahdianpari, Lindi Quackenbush,\nSarina Adeli, and Brian Brisco. 2020. “Google Earth\nEngine for Geo-Big Data Applications: A\nMeta-Analysis and Systematic Review.” ISPRS Journal of\nPhotogrammetry and Remote Sensing 164 (June): 152–70. https://doi.org/10.1016/j.isprsjprs.2020.04.001.\n\n\nTeixeira, Fabio. 2022. “In Brazil, Satellites Help\nScientists Target Amazon Forest Losses.” Thomas\nReuters Foundation News.\nhttps://news.trust.org/item/20220406135051-ut6cr/.\n\n\nTralli, David M., Ronald G. Blom, Victor Zlotnicki, Andrea Donnellan,\nand Diane L. Evans. 2005. “Satellite Remote Sensing of Earthquake,\nVolcano, Flood, Landslide and Coastal Inundation Hazards.”\nISPRS Journal of Photogrammetry and Remote Sensing, Remote\nSensing and Geospatial Information for\nNatural Hazards Characterization, 59 (4): 185–98. https://doi.org/10.1016/j.isprsjprs.2005.02.002.\n\n\nTrias-Sanz, Roger, Georges Stamon, and Jean Louchet. 2008. “Using\nColour, Texture, and Hierarchial Segmentation for High-Resolution Remote\nSensing.” ISPRS Journal of Photogrammetry and Remote\nSensing 63 (2): 156–68. https://doi.org/10.1016/j.isprsjprs.2007.08.005.\n\n\nWilber, Jared. 2022a. “Precision and Recall.”\nMLU-Explain. https://mlu-explain.github.io/precision-recall/.\n\n\n———. 2022b. “ROC and AUC.”\nMLU-Explain. https://mlu-explain.github.io/roc-auc/.\n\n\nYerushalmy, Jonathan. 2023. “Gaza Before and After: Satellite\nImages Show Destruction Following Israeli\nAirstrikes.” The Guardian, October."
  },
  {
    "objectID": "02_portfolio.html#the-presentation",
    "href": "02_portfolio.html#the-presentation",
    "title": "2  Portfolio",
    "section": "2.1 The presentation",
    "text": "2.1 The presentation"
  },
  {
    "objectID": "index.html#about-me",
    "href": "index.html#about-me",
    "title": "CASA0023 - Remote Sensing Learning Diary",
    "section": "About me",
    "text": "About me\n\n\n\nSoki Kimura - at Karatsu, Saga, Japan during a first real trip on a motorcycle after I got my license. This place has the best squids in the world.\n\n\nI am Soki Kimura, a master’s student at the Bartlett Centre for Advanced Spatial Analysis at University College London.\nI am interested in the designing and analysing of cities, with some experience as a planner in the public sector.\nMy interest toward cities probably started building up playing SimCity 4 - maybe when I was 7 years old. Building cities and looking at how well it was performing, zoning areas and trying to make private development happen, all of that topped up with pleasing music was my first interaction with spatial data.\nNever had I imagined I will still be playing around with spatial data 20 years later!"
  },
  {
    "objectID": "index.html#about-this-website",
    "href": "index.html#about-this-website",
    "title": "CASA0023 - Remote Sensing Learning Diary",
    "section": "About this website",
    "text": "About this website\nThis is the learning diary for the CASA0023 Remote Sensing Cities and Environment module led by Dr. Andy Maclachlan.\nI will summarise the content of each week throughout the module, followed by few papers that apply the methodology. I will make personal reflections for each week’s content, trying as much as I can to relate to my personal experience or knowledge."
  },
  {
    "objectID": "01_intro.html#summary",
    "href": "01_intro.html#summary",
    "title": "1  Getting Started with Remote Sensing",
    "section": "1.1 Summary",
    "text": "1.1 Summary\nRemote Sensing: acquiring information from a distance (NASA 2019). There is a huge number of satellites carrying sensors that observe the earth.\n\n1.1.1 Characteristics of Satellite Imagery\nThe data that can be acquired from remote sensing is interesting! Few things that caught my attention:\n\nThe availability of data is amazing. The whole globe is covered every day, including areas impossible to be accessed by humans.\nUsing different wavelengths let us see what the human eye cannot. For instance, near infrared lets us differentiate vegetation from other “green stuff”.\n\n\n\n\nA false color image of NYC and the Hudson Valley using NIR bands, where vegetation appears red\n\n\n\n\n1.1.2 Interaction with atmosphere\nThe atmosphere impacts the satellite image, which we must account for. There are a few “windows” that electromagnetic waves with certain wavelengths can pass through: the most notable one being the optical window, and the radio window. We must also consider the scattering for the wavelengths that do pass by. We have to consider these interactions with atmosphere when we use data, but nowadays software does the hard work for us.\n\n\n\nAtmospheric Windows from GISGeography (2017)\n\n\nMeanwhile the astronomers have put their telescope in outer space to avoid the atmosphere.\n\n\n\nThe Hubble Space Telescope in outer space\n\n\n\n\n1.1.3 Resolutions of Satellite Imagery\nResolutions for satellite imagery has different dimensions of resolutions, something I did not think about before.\n\nSpatial Resolution: the size of each pixel within the image. Higher resolution means you can see the surface of the earth with more detail. This was the only type that came into my mind before this lecture.\nSpectral Resolution: the number of bands of different wavelengths an observation has. This includes visible light such as red, green, and blue, but spans into wavelengths undetectable by the human eye. Some hyperspectral images have more than 200! The better spectral resolution, the better it is to differentiate the materials.\nRadiometric Resolution: is the amount of information in each pixel, the higher the value the more potential values it has. Distinguishing slight changes are made possible with high radiometric resolution. I feel this is a similar idea to the color depth, where 24-bit colors have more variety than 8-bit colors.\nTemporal Resolution: the frequency the data is collected, or the frequency the satellite flies over each area. SENTINEL-2 has 2 satellites, doubling the temporal resolutions compared to having only one.\n\nThese often come with trade-offs (detailed data comes at the cost of lower frequency) therefore we must be aware which is important for analysis that we make."
  },
  {
    "objectID": "01_intro.html#applications",
    "href": "01_intro.html#applications",
    "title": "1  Getting Started with Remote Sensing",
    "section": "1.2 Applications",
    "text": "1.2 Applications\n\n1.2.1 Research\nThe applications of satellite imagery is huge! Here are a few that caught my eye.\nOne example is that of MapBiomas Alerta (MapBiomas n.d.) tracking deforestation and alerting stakeholders upon detection. According to Teixeira (2022) where the project is reviewed, the main goal of this project is to add to the transparency of the status of deforestation. The change in the political situation in Brazil has also contributed to the increase in deforestation, and the team supports local authorities to prosecute illegal deforestation.\nRemote sensing can also be used to predict and assess natural hazards. Gillespie et al. (2007) has summarised the ways damage from disasters including earthquakes, floods, and fires, can be observed from satellite imagery. They have also proposed ways to identify potential threats happening in the future. They have successfully observed the aftermath of disasters by capturing collapsed buildings or flooded areas, but their methodology on prediction of disasters may need further refinement. Limitations include the spatial and temporal resolution of available data, and atmospheric interference affecting the quality of data. Assessment of damage can also be applied to conflicts, one of the examples being Yerushalmy (2023) investigating the Gaza Strip.\nThese research show the potential of satellite imagery, making available data for all areas of the globe, regardless of the inaccessibility the area has. Conflict areas are a great example of this, where actual exploration is basically impossible. The limitation lies in the spatial resolution, where the granularity of the data directly affects the resolution of analysis. Temporal resolution makes near-real-time interaction impossible. A detailed analysis always need on-site observations, but making use of universally available data may be useful for early reactions.\n\n\n1.2.2 Personal Analysis on New York City\nFor my first satellite imagery analysis, I chose New York City, USA.I have sampled areas with different land cover and checked the difference in reflectance, and compared observations from Landsat-8 and Sentinel-2. The following places were sampled, representing each of the land coverages.\n\n\n\n\n\n\n\nLand Coverage\nSampled Place\n\n\n\n\nForest\nNorthwest Forest, Van Cortlandt Park, Bronx\n\n\nGrass\nParade Ground, Van Cortlandt Park, Bronx\n\n\nHigh Reflection Urban\nIndustry City, Brooklyn\n\n\nUrban\nMidtown East, Manhattan\n\n\nWater\nJacqueline Kennedy Onassis Reservoir, Central Park, Manhattan\n\n\n\n\n\n\nMap of sampled places.\n\n\nI extracted pixels from Sentinel and Landsat within these areas, and analysed the spectral profiles from both.\n\n\n\nSpectral profiles from Sentinel and Landsat\n\n\nWavelengths for each band is as follows in the two satellites, and we can see some slight differences.\n\n\n\nBand\nSentinel\nLandsat\n\n\n\n\n1 (Blue)\n492.4\n490\n\n\n2 (Green)\n559.8\n560\n\n\n3 (Red)\n664.6\n665\n\n\n4 (NIR)\n832.8\n842\n\n\n5 (SWIR)\n1613.7\n1610\n\n\n6 (SWIR)\n2202.4\n2190\n\n\n\nOverall, we can see a similar pattern observed in the 2 satellite projects, with subtle differences observed in the infrared areas.\nBuilding on this difference, I feel it is possible to identify and classify the different land usages from satellite imagery."
  },
  {
    "objectID": "01_intro.html#reflection",
    "href": "01_intro.html#reflection",
    "title": "1  Getting Started with Remote Sensing",
    "section": "1.3 Reflection",
    "text": "1.3 Reflection\nThe first impression on the term remote sensing was that we are going to do something about sensors located at the surface (which is what I assume the people at the MSc Connected Environments are doing). Nonetheless, the applications of satellite imagery is very fascinating!\n\n1.3.1 An Additional Note\nRecent news from my home country of Japan, hit by an earthquake and tsunami on New Year’s Day, featured satellite imagery showing the damage by the disaster. Japan is situated within the Pacific Ring of Fire, and is under an emergent threat of a massive earthquake hitting Tokyo and the most of the Pacific region (70% chance within the next 30 years). Now that I actually saw remote sensing in action, I feel motivated toward learning to make use of this technology.\n\n\n\n\n\nGillespie, Thomas W., Jasmine Chu, Elizabeth Frankenberg, and Duncan Thomas. 2007. “Assessment and Prediction of Natural Hazards from Satellite Imagery.” Progress in Physical Geography: Earth and Environment 31 (5): 459–70. https://doi.org/10.1177/0309133307083296.\n\n\nGISGeography. 2017. “The Atmospheric Window in Remote Sensing.” GIS Geography. https://gisgeography.com/atmospheric-window/.\n\n\nMapBiomas. n.d. “MapBiomas Alerta.” https://alerta.mapbiomas.org/en. Accessed February 7, 2024.\n\n\nNASA. 2019. “What Is Remote Sensing?  Earthdata.” Backgrounder. https://www.earthdata.nasa.gov/learn/backgrounders/remote-sensing; Earth Science Data Systems, NASA.\n\n\nTeixeira, Fabio. 2022. “In Brazil, Satellites Help Scientists Target Amazon Forest Losses.” Thomas Reuters Foundation News. https://news.trust.org/item/20220406135051-ut6cr/.\n\n\nYerushalmy, Jonathan. 2023. “Gaza Before and After: Satellite Images Show Destruction Following Israeli Airstrikes.” The Guardian, October."
  },
  {
    "objectID": "03_rs_data.html#summary",
    "href": "03_rs_data.html#summary",
    "title": "3  Remote Sensing Data Manipulation",
    "section": "3.1 Summary",
    "text": "3.1 Summary\nIn order to use the remote sensing data most effectively, data manipulation is desired. This comes in two perspectives: correcting data to remove the impacts from unnecessary factors, and joining and enhansing data.\n\n3.1.1 Data Correction\nThe raw data fetched from satellites are subject to interference by environmental factors. Data correction is required to correctly analyse the satellite imagery obtained. The following explains the various aspects of data correction.\nThankfully, the contemporary satellite imagery prouducts come in ARD: Analysis Ready Data so I am expecting we don’t need to worry too much about these jargons.\n\nAtmospheric Correction\nRemoving the effect of atmosphere which absorbs and scatters light, leading to haze (reduced contrast of image) and the adjacency effect (nearby pixels bleeding into each other).\n\nRelative Correction\nRelative correction normalises intensities of bands within a single iamge or within multiple dates. The dark object subtraction (DOS) method assumes the darkest value in the image displays the effect of atmosphere, and subtracts that value from the whole image, leaving the actual surface reflectance value. Other methods such as pseudo-invariant features are proposed as well.\nWhile these simple methods can calibrate values within one imagery, the relative nature does not allow for comparison between different imagery - this is where absolute correction comes into play.\n\n\nAbsolute Correction\nCalculates the scaled absolute value for surface reflectance, enabling comparison between different places on earth. Multiple models are proposed, or a field work can be conducted to make the satellite observe the empirical line of spectrometer you laid out in the wild.\n\n\n\nExample of atmospheric correction. Image from Liang and Wang (2020)\n\n\n\n\n\nGeometric Correction\nGeometric correction is manipulation to fit the earth observation data into a reference system. This is similar to the process of georeferencing analog maps (Skopyk 2021).\nThis is achieved by taking several reference points (Ground Control Points) on the imagery where the actual coordinates are known, and interpolating the areas in between (Jensen 2016).\n\n\n\nReference points on satellite imagery and the gold standard map (image from SEOS)\n\n\nThe actual mapping is done backwards, where points on the imagery are calculated from coordinates on the gold standard map, so that every point on the study area has its corresponding point on the imagery. This is modelled as follows.\n\\[\n\\begin{cases}\nx^i = a_0 + a_1x + a_2y+\\epsilon_i \\\\\ny^i = b_0 + b_1x + b_2y+\\epsilon_i \\\\\n\\end{cases}\n\\]\n\n\\((x^i, y^i)\\): coordinates on the satellite imagery\n\\((x, y)\\): coordinates on the gold standard map\n\nOrthorectification or topographic correction is one aspect of this, correcting the image so that it is viewed at nadir. One method is cosine correction, where the radiance is calculated as:\n\\[\nL_H = L_T \\frac{\\cos \\theta_O}{\\cos i}\n\\]\n\n\\(L_T\\): radiance (TOA) from sloped terrain\n\\(\\theta_O\\): zenith angle of the sun\n\\(i\\): sun’s incidence angle (angle between rays and normal of surface)\n\nThis will be important especially if using imagery from aircrafts flying at relatively low altitude compared to satellites.\n\n\nRadiometric Calibration\nThe original satellite imagery stores data using digital numbers (DN) which has no units! This needs to be translated into values with meaningful units.\n\n\n\n3.1.2 Data Joining and Enhancement\nWe do not need to use the data as is, but are open to wrangling data for analysis as well.\n\nJoining\nJoining is the procedure of combining separate images from different data sources to cover the whole study area. Also called mosaicking. The algorithm behind this is called a histogram matching algorithm, where the 2 images are given similar brightness values.\n\n\nEnhancements\nImages may be enhanced to improve the appearance or results of analysis.\nThere are many enhancement methods. One simple method is contrast enhancement where image band not making use of the whole spectrum range are expanded so that the contrast is maximised.\n\n\n\nExample of image stretch by contrast enhancement. Image from EarthLab (2018)\n\n\n\n\n3.1.2.1 Ratio enhancements\nThe one I found interesting was ratio enhancements, where the manipulation between different bands enabling us to gain insights from the results.\nOne example of this is the normalised difference vegetation index (NDVI), where a high value indicates healthy vegetation. The calculation is as follows, and I was surprised how simple this value could be calculated!\n\\[\n\\text{NDVI} = \\frac{\\text{NIR}-\\text{red}}{\\text{NIR}+\\text{red}}\n\\]\nI have applied this to a dataset from Landsat 8 for southeast England, where I merged 2 datasets to cover a wider area.\n\n\n\nNDVI of Southeast England\n\n\nIt is interesting to see that you can make out the patch with low value on the right hand side which is Central London, and that you can see the vegetation has a range in its healthiness; if you look at the optical imagery it is hard to tell the differences between healthy and unhealthy vegetation.\nLet’s zoom in to the London Borough of Camden - home to two major patches of urban green space: Hampstead Heath and Regent’s Park.\n\n\n\nNDVI of the London Borough of Camden\n\n\nYou can easily make out Regent’s Park and Primrose Hill at the bottom left and Hampstead Heath at the top, which are home to the healthiest patches of vegetation in this area. Smaller patches have smaller NDVI value, indicating they are unhealthier compared to that of parks. If you compare the scale to the bigger picture above, you can see that the maximum value is lower in Camden. Understandable, but it is great you can see the difference!\n\n\n3.1.2.2 Other techniques\nOther enhancement techniques covered include the following:\n\nFiltering sharpens (high-pass) or averages (low-pass) surrounding cells in order to capture particular characteristics\nPCA is a method of dimension reduction, taking multiple bands and combining them into a smaller number of bands.\nTexture considers the difference of values between moving windows, allowing for edges of certain object to be clearer.\n\nI calculated the texture value using the red band for the London Borough of Camden, and the results are shown below:\n\n\n\nTexture of Camden\n\n\nI must admit it is difficult to make out individual differences, but you can see the areas around King’s Cross, St. Pancras and Euston stations having a smaller texture value. It seems to be successfully identifying the different characteristics of urban landscape!\n\n\n\n3.1.3 Glossary\nThere were a lot of new terminology introduced in this session.\n\n3.1.3.1 Data Correction Vocabulary\n\nnadir: looking straight down, not at an angle.\nzenith: point vertically above observer. Zenith Angle is the angle between object and zenith.\nincident angle: the angle between the normal of the sloped terrain and the sun. If horizontal, this matches the zenith angle.\nazimuth: the compass angle of the sun, measured from the north. Same idea with runway names!\n\n\n\n3.1.3.2 Data Enhancement Vocabulary\n\nImage Stretch: expanding the range of bands so that it utilises the whole range of values allowed\nTexture: calculating the value across a certain window and comparing with adjacent values, allowing to identify edges of features\nPCA: a method of dimension reduction"
  },
  {
    "objectID": "03_rs_data.html#applications",
    "href": "03_rs_data.html#applications",
    "title": "3  Remote Sensing Data Manipulation",
    "section": "3.2 Applications",
    "text": "3.2 Applications\nI will look at 2 papers that measure the accuracy of image segmentation using different color bands and methodologies.\nTrias-Sanz, Stamon, and Louchet (2008) explores the usage of colour bands and textures to identify objects within an image. Using multiple bands, the paper attempts to optimise the best parameter that successfully identifies different objects in an imagery of a rural area. The paper addresses the varying scale of focus in the project, whether to focus on individual trees within a field or the field as a total. It also discusses the multiple ways of data enhancement, and tries to optimise which parameters and transformed channels should be used to achieve the best results for object segmentation.\nKupidura (2019) analysed multiple methods of texture analysis to compare the accuracy of identifying different land cover types. 3 types of texture extraction was considered, the GLCM (Gray Level Co-occurance Matrix), Laplace Filters and Granulometric Analysis. The first two methods seem to be more popular among remote sensing analysis, and a third method which is less known. The results indicate the granulometric analysis is the best methodology in terms of identifying different usages from satellite imagery, although I get the feeling that the authors are trying to promote this methodology that they have been using for previous research, since the merit of the granulometric analysis derives from mitigation of the edge effect, a problem which the two other methods have in common. Maybe comparing different methodologies to overcome the edge effect would have been more persuasive (or, maybe this in itself is quite an achievement. I might need more knowledge in this field to answer that question.)\nBoth of the papers address the accuracy of image segmentation using many possible index channels, where we try to identify the objects that appear in the satellite imagery. The first paper has a focus on tweaking parameters within a single methodology, while the second paper focuses on choosing the correct method given a remote sensing dataset. The 10-year difference between these papers might have seen a dramatic advance in technology, but I have the impression that trying to make image segmentation as accurate as possible is an essential part of EO data analysis."
  },
  {
    "objectID": "03_rs_data.html#reflection",
    "href": "03_rs_data.html#reflection",
    "title": "3  Remote Sensing Data Manipulation",
    "section": "3.3 Reflection",
    "text": "3.3 Reflection\nData correction is the first thing we have covered this week, and I cannot believe analysis ready data is provided after all this manipulation. Although I might not do the data correction myself, I should be prepared to understand the differences between the multiple products of satellite imagery, and be able to choose which one I should be using for analysis.\nI feel very thankful for the people trying to improve the algorithm behind analyses; which I could not get my head around. The idea of taking multiple bands and adjacent pixels are interesting enough already, and trying to make out different features are wonderful!\nAfter a few weeks, I am starting to understand the variety of things you can do using satellite imagery. I did not have the idea of taking adjacent values and calculate indeces that act as indicators of change among the different datasets, which helps differentiate between objects within the imagery. I am feeling I should deep-dive into the different indeces developed by other researchers, to see what can be highlighted using simple combinations of different bands!\n\n\n\n\nEarthLab. 2018. “Learn to Use NAIP Multiband Remote Sensing Images in Python.” Earth Data Science - Earth Lab. https://www.earthdatascience.org/courses/use-data-open-source-python/multispectral-remote-sensing/intro-naip/.\n\n\nJensen, John R. 2016. Introductory Digital Image Processing: A Remote Sensing Perspective / John R. Jensen (University of South Carolina). 4th edition. Pearson Series in Geographic Information Science. Glenview, IL: Pearson Education, Inc.\n\n\nKupidura, Przemysław. 2019. “The Comparison of Different Methods of Texture Analysis for Their Efficacy for Land Use Classification in Satellite Imagery.” Remote Sensing 11 (10): 1233. https://doi.org/10.3390/rs11101233.\n\n\nLiang, Shunlin, and Jindi Wang, eds. 2020. “Chapter 4 - Atmospheric Correction of Optical Imagery.” In Advanced Remote Sensing (Second Edition), 131–56. Academic Press. https://doi.org/10.1016/B978-0-12-815826-5.00004-0.\n\n\nSkopyk, Brad. 2021. “Georeferencing Historical Maps.” ArcGIS StoryMaps. https://storymaps.arcgis.com/stories/dd75d0398f7d4ded924d303161895b8b.\n\n\nTrias-Sanz, Roger, Georges Stamon, and Jean Louchet. 2008. “Using Colour, Texture, and Hierarchial Segmentation for High-Resolution Remote Sensing.” ISPRS Journal of Photogrammetry and Remote Sensing 63 (2): 156–68. https://doi.org/10.1016/j.isprsjprs.2007.08.005."
  },
  {
    "objectID": "03_rs_data.html#reference",
    "href": "03_rs_data.html#reference",
    "title": "3  Remote Sensing Data Manipulation",
    "section": "3.4 Reference",
    "text": "3.4 Reference"
  },
  {
    "objectID": "03_rs_data.html",
    "href": "03_rs_data.html",
    "title": "3  Data Manipulation",
    "section": "",
    "text": "4 Remote Sensing Data Manipulation"
  },
  {
    "objectID": "04_policy.html#summary",
    "href": "04_policy.html#summary",
    "title": "4  Policy Implications",
    "section": "4.1 Summary",
    "text": "4.1 Summary\nThe 2 documents I chose for this week’s practical are:\n\nOneNYC 2050\nG7 Sustainable Urban Development Ministers’ Meeting Communiqué\n\n\n4.1.1 OneNYC 2050\nOneNYC 2050 addresses the challenges that come along for a sustainable, resilient and inclusive growth toward 2050, as one of the very few cities with growing population.\nThe visions proposed in this document are as follows (excerpt from pp.46-47):\n\nA vibrant democracy\nAn inclusive economy\nThriving neighbourhoods\nHealthy lives\nEquity and excellence in education\nA livable climate\nEfficient mobility\nModern infrastructure\n\nThe challenges addressed in this policy document is (p24):\n\nRising Unaffordability\nEconomic Insecurity\nWealth and Health Disparities\nA Climate Emergency\nFailing Infrastructure and Shifting Needs\nThreats to Democracy\n\nWe can immediately see that most of the factors addressed are related to socio-economic factors, which seem to be difficult to observe from above the atmosphere.\nImpacts of climate change and infrastructure is one topic that seems to be more manageable by observing the morphology of the urban landscape.\nApplication could be in the following areas:\nStrengthening resilience toward infrastructure\nThis is a pretty straightforward one I guess, where we might be able to model potential hazards, and use the current data on infrastructure and elevation to measure the potential damage of disasters. Long-term observations may be used to measure the change in temperature, precipitation, and so forth.\nSocio-economic features through earth observation\nSince there is so much focus on the apparently unphysical characteristics of the urban landscape, I feel I should ask the question: is there a way to address socio-economic issues using infrastructure?\nWe must recall New York is the site of the world-famous dispute between the great activist Jane Jacobs and the master builder Robert Moses (Paletta 2016), which sparked off from the plans for the Lower Manhattan Expressway which was to destroy the local environment. Some potential strategies may be: 1) how much amenity space demolished? (There should have been victims even after Washington Square Park was saved), and 2) are infrastructure casting negative impacts on the livability of the city (like, shadows cast on neighbourhoods?)\nWe will explore methodology in the application section.\n\n\n4.1.2 G7 Sustainable Urban Development Ministers’ Meeting Communique\nNow I will look at an international agreement with focus on cities.\nThe outcome document for the G7 Sustainable Urban Development Ministers’ Meeting in Takamatsu, Kagawa, the second G7 ministerial meeting by ministers in charge of urban policies, held during the Japanese presidency of G7 in 2023.\nA niche policy document, yes. I specifically chose this one because:\n\nit’s very recent (July 2023)\nit is drawing from other international agendas\nI was involved in the editing process just before starting my master’s course\n\nThis addresses the issues we have in contemporary cities (in the G7 context), and I would want to see if anything can be related to remote sensing in general.\nThis document addresses 3 overarching themes, having a total of 54 paragraphs:\n\nNet-zero and resilient cities\nInclusive cities\nDigitalisation in cities\n\nThe few topics I thought remote sensing has a part to play:\nParagraph 15 - Biodiversity in cities:\n\n… Green and blue space in cities is too often at risk of being lost due to urban development that is not biodiversity-inclusive; built areas are expanding faster than the population is growing, and unplanned, uncoordinated urban development around cities is consuming the natural environment and green and blue space. …\n\nThis statement implies the problem lies in:\n\nthe expansion of built-up areas faster than population growth\nurban sprawl destroying green and blue spaces\n\nand that these phenomena lead to a loss in biodiversity.\nParagraph 18 - Integration of land use policy and transport policy:\n\nIntegrating land use policies with transport policies is a good example of policy coordination.\n\nThis comes down to building up areas that are accessible by public transport, the idea of compact plus network the Japanese are so keen about.\nThe current situation of urban sprawl and loss of urban green and blue spaces may be observed using EO data. Using the long-term observation data (i.e. Landsat Project) we will be able to observe the growth of built-up areas over time, what type of land (vegetation, water, or else) it has consumed, and if it was a sprawl development or a more controlled one.\nThe digitalisation part has a lot of interesting statements regarding open data, but not much was mentioned regarding EO data."
  },
  {
    "objectID": "04_policy.html#applications",
    "href": "04_policy.html#applications",
    "title": "4  Policy Implications",
    "section": "4.2 Applications",
    "text": "4.2 Applications\nI have identified a few topics that remote sensing may play a role.\n\nResilience against natural hazards\nGrowth of urban built-up areas and impact on green and blue spaces\nLand use and match with transportation\nSocio-economic factors and urban landscape\n\nThe following are potential methodology that may be suitable for the analysis of areas.\n\n4.2.1 Resilience against natural hazards\nGhaffarian, Kerle, and Filatova (2018) has provided a review on proxy parameters for disaster risk that can be extracted from satellite imagery. These include building height and material for vulnerability against earthquakes, road network analysis for assessment of transport networks, and flood water levels and debris detection for flood risk assessment. Rong et al. (2020) has used DSM (Digital Surface Model) data for hydrodynamic simulation of flood risks, a method where Gillespie et al. (2007) has indicated but faced limitations of spatial resolution at that age.\nData on the spatial characteristics of cities can be a useful way to assess flood and other disaster risk.\n\n\n4.2.2 Built-up areas and green and blue spaces\nLand cover mapping is one of the major applications of earth observation data (Ming et al. 2016), and a lot of work has been done to achieve this.\nMing et al. (2016) has used random forest methods to classify natural land cover using satellite data, where they have classified usages with high averages, but limitations lie in its accuracy to identify urban areas compared to natural land cover.\nGupta et al. (2012) has proposed the Urban Neighborhood Green Index - an index of quality of green space within built-up areas using NDVI (vegetation detection), building height estimation (shadow length) and classification of vegetation (using image classification techniques).\nThese methodologies can be used for approximating the available green space within the urban landscape, and an index will allow for comparisons between cities.s\n\n\n4.2.3 Land use and infrastructure\nThis is a minor revision of the above: comparing the built-up area with the infrastructure is a potential way to identify the disadvantages of urban sprawl. An interesting approach by Shao et al. (2021) combined the identification of built-up areas using remote sensing and social media activity location data, and suggested the relative inactivity in sprawl areas is related to the lack of telecommunication infrastructure. By comparing the usages of infrastructure (such as transport OD data) and urban built up areas, we may be able to draw some interesting characteristics of urban areas.\n\n\n4.2.4 Socio-economic factors\nEvaluation of socio-economic factors are a difficult topic just by using earth observation data.\nFew suggestions include:\n\nNetwork analysis of transport infrastructure\nNight-time lights data for economic analysis\nBuilding footprints\nReflection levels (high-level of reflection indicate industrial land use)\nBuilding shadows? Shaded by infrastructure?\n\nI might need some extra reading on this, but I feel it will be an interesting topic to explore. Urban morphology and the architecture is heavily dependent on the local environment and culture, so unsure if one way of data observation can contribute to the whole picture."
  },
  {
    "objectID": "04_policy.html#reflections",
    "href": "04_policy.html#reflections",
    "title": "4  Policy Implications",
    "section": "4.3 Reflections",
    "text": "4.3 Reflections\nHaving experience in the public sector, I sort of understand the difficulty of making policies and multilateral agreements. Even if stakeholders agree on the big picture, the individual statements do not necessarily align their interests, thus leading to dispute. When it comes to technical methodology, I doubt those decision-makers have the capability to debate in the first place.\nNonetheless it is undeniably an important topic, so I believe this is where experts come into play, actually assessing policies based on the rules those policy documents have set up.\nI feel that remote sensing data has the potential to overcome some of the problems; one major cause of dispute among different stakeholders is the varying definitions and interpretations of topics on the international agenda. The nature of satellite imagery where all parts of the globe are sensed equally may contribute to solving common causes of disagreement such as lack of data, different people having different criteria and explanations, the problem of cost, and more.\nThis was a very good exercise for me!\n\n4.3.1 Additional notes\nMy other interest lies in the field of disaster risk reduction, but couldn’t dig in deep enough to make it a full entry on the learning diary.\nThe overarching document recently is the Sendai Framework for Disaster Risk Reduction, which aims to substantially reduce the risk and losses from disasters.\n7 targets are defined:\n\n\nsubstantially reduce global disaster mortality by 2030\nsubstantially reduce number of affected people globally\nreduce direct disaster economic loss\nsubstantially reduce disaster damage to critical infrastructure and disruption of basic services (health, education)\nsubstantially increase the number of countries with DRR strategy\nsubstantially enhance int’l cooperation to developing countries through adequate and sustainable support\nsubstantially increase availability of and access to multi-hazard early warning\n\n\nRemote sensing may come into play for assessment of affected people, potential risk. Possible datasets are:\n\nDSM models\npopulation density data\n\nThe risk assessment will be the hard part. Some papers (Geiß and Taubenböck 2013; Tralli et al. 2005) do suggest earthquakes and volcano risks can also be assessed using satellite imagery, but need to look in deeper.\n\n\n\n\nGeiß, Christian, and Hannes Taubenböck. 2013. “Remote Sensing Contributing to Assess Earthquake Risk: From a Literature Review Towards a Roadmap.” Natural Hazards 68 (1): 7–48. https://doi.org/10.1007/s11069-012-0322-2.\n\n\nGhaffarian, Saman, Norman Kerle, and Tatiana Filatova. 2018. “Remote Sensing-Based Proxies for Urban Disaster Risk Management and Resilience: A Review.” Remote Sensing 10 (11): 1760. https://doi.org/10.3390/rs10111760.\n\n\nGillespie, Thomas W., Jasmine Chu, Elizabeth Frankenberg, and Duncan Thomas. 2007. “Assessment and Prediction of Natural Hazards from Satellite Imagery.” Progress in Physical Geography: Earth and Environment 31 (5): 459–70. https://doi.org/10.1177/0309133307083296.\n\n\nGupta, Kshama, Pramod Kumar, S. K. Pathan, and K. P. Sharma. 2012. “Urban Neighborhood Green Index – A Measure of Green Spaces in Urban Areas.” Landscape and Urban Planning 105 (3): 325–35. https://doi.org/10.1016/j.landurbplan.2012.01.003.\n\n\nMing, Dongping, Tianning Zhou, Min Wang, and Tian Tan. 2016. “Land Cover Classification Using Random Forest with Genetic Algorithm-Based Parameter Optimization.” Journal of Applied Remote Sensing 10 (3): 035021. https://doi.org/10.1117/1.JRS.10.035021.\n\n\nPaletta, Anthony. 2016. “Story of Cities #32: Jane Jacobs v Robert Moses, Battle of New York’s Urban Titans.” The Guardian, April.\n\n\nRong, Youtong, Ting Zhang, Yanchen Zheng, Chunqi Hu, Ling Peng, and Ping Feng. 2020. “Three-Dimensional Urban Flood Inundation Simulation Based on Digital Aerial Photogrammetry.” Journal of Hydrology 584 (May): 124308. https://doi.org/10.1016/j.jhydrol.2019.124308.\n\n\nShao, Zhenfeng, Neema S. Sumari, Aleksei Portnov, Fanan Ujoh, Walter Musakwa, and Paulo J. Mandela. 2021. “Urban Sprawl and Its Impact on Sustainable Urban Development: A Combination of Remote Sensing and Social Media Data.” Geo-Spatial Information Science 24 (2): 241–55. https://doi.org/10.1080/10095020.2020.1787800.\n\n\nTralli, David M., Ronald G. Blom, Victor Zlotnicki, Andrea Donnellan, and Diane L. Evans. 2005. “Satellite Remote Sensing of Earthquake, Volcano, Flood, Landslide and Coastal Inundation Hazards.” ISPRS Journal of Photogrammetry and Remote Sensing, Remote Sensing and Geospatial Information for Natural Hazards Characterization, 59 (4): 185–98. https://doi.org/10.1016/j.isprsjprs.2005.02.002."
  },
  {
    "objectID": "04_policy.html#application",
    "href": "04_policy.html#application",
    "title": "4  Policy Implications",
    "section": "4.2 Application",
    "text": "4.2 Application"
  },
  {
    "objectID": "index.html#how-this-website-is-made",
    "href": "index.html#how-this-website-is-made",
    "title": "CASA0023 - Remote Sensing Learning Diary",
    "section": "How this website is made",
    "text": "How this website is made\nThis website was rendered using Quarto enabling us to create websites from RStudio environments! I have used this for creating assignment reports, and I am amazed how you can easily create websites from a markdown document."
  },
  {
    "objectID": "05_gge.html#summary",
    "href": "05_gge.html#summary",
    "title": "5  Google Earth Engine",
    "section": "5.1 Summary",
    "text": "5.1 Summary\nGoogle Earth Engine is a cloud-based online platform providing access to huge amounts of analysis-ready data and high-performing computing resources (Gorelick et al. 2017). The ability to access to both large data and the computational power capable of handling it this enables large-scale analysis that is otherwise impossible using individual computers. It also allows to publish applications that end-users can directly interact with. This vastly increases the potential audience and the data available to them, having the potential to change decisions in many levels.\nIn terms of analysis, one important factor of Google Earth Engine is that we no longer have to worry too much about spatial resolutions and merging data. The spatial resolution is defined by the output resolution, and it automatically aggregates the data to match the desired output. The datasets organised into image collections enable simple filtering, merging and aggregating of multiple imageries.\n\n5.1.1 An example using Delhi, India\nMy first data manipulation using Google Earth Engine was for Delhi, India - following the instructions on the module notebook.\nThe workflow was a very straightforward process - even if it did involve a bit of JavaScript - and I did not need to wait forever for my computer to process the huge raster files!\nExtracting the datasets that cover the whole of the study area, while considering the cloud cover and period of observation, can be done by selecting data using code, easily achieving the following results.\n\n\n\nLandsat 8 imagery for tiles that overlap with the New Delhi area. The black area indicates the study area.\n\n\nMerging and clipping data did not take very long (much shorter compared to the local environment!), and we could calculate the texture component of the whole Delhi area.\n\n\n\nTexture of the Delhi area. Pink areas indicate areas with high difference compared to others - indicating the existence of buildings.\n\n\nThis accessibility to the dataset is a massive improvement compared to manually locating and downloading files, using software to analyse, and producing outputs, all requiring some amount of effort."
  },
  {
    "objectID": "05_gge.html#applications",
    "href": "05_gge.html#applications",
    "title": "5  Google Earth Engine",
    "section": "5.2 Applications",
    "text": "5.2 Applications\nApplications of Google Earth Engine spreads across various fields and areas, with high volumes of research published in the fields of agriculture, hydrology, and land use analysis; the majority using optical imagery data (Tamiminia et al. 2020; Amani et al. 2020).\nOne of these research application is done by Sidhu, Pebesma, and Câmara (2018), where they have explored the land use change in the island nation of Singapore. This research observes two phenomena: the dredging (expansion of land into the ocean by placing sand on the ocean floor) seen in the coastal industrial area, and the preservation of the natural environment in the Central Catchment Reserve (CCR). This was observed using the Enhanced Vegetation Index (EVI), calculated as follows:\n\\[\n\\text{EVI} =  \\frac{2.5\\left(\\rho_{\\text{nir}}-\\rho_{\\text{red}}\\right)}{\\rho_{\\text{nir}}+6\\rho_{\\text{red}}-7.5\\rho_{\\text{blue}}+1}\n\\]\nwhere \\(\\rho\\) is the surface reflectance for each band.\nThis index was used due to its higher resistance to atmospheric noise, and is an index used to measure the vegetation level from satellite imagery. The strength of the Google Earth Engine was used in order to calculate the time series for the EVI in the study areas for a period of 5 years, and comparing 2 data sources (Landsat and MODIS). The observations at the CCR has shown there is a seasonal difference in the EVI levels due to the change of vegetation conditions, highlighting the monsoon season observed in this part of the world. However, the observations for the dredging area saw no significant sign of dredging captured from this EVI index. The EVI focuses on detecting vegetation, and it was difficult to detect the change from water (ocean) to bare soil (dredging). Although this research aimed to explore the potential of GGE, it is highlighting the need to consider indeces carefully to meet the needs of each research. Even with the high computational power and petabytes of data, the researchers must be aware of the processing tool we are using.\nGoogle Earth Engine is capable of building spatial applications using Google Earth Engine Apps, which is another field where Google Earth Engine is spreading its use-cases. One example is the OFF!Cast Mosquito Forecast created by a pesticide brand OFF! predicting mosquito outbreaks within the United States. The weather data accumulated by Google Earth Engine is combined with a model comparing weather conditions to mosquito activities (Buonfantino 2022). This allows end-users to get the mosquito forecast for their area of interest by querying using their ZIP code. Although this may be a simplified analysis aimed to enhance the consumption of their products, it shows the possibility of highly-processed spatial data being tailor-made to fit each individuals’ interests.\nThese are 2 of the various applications made by Google Earth Engine. From within the academic realm to reaching out to individual end-users who are not expected to be familiar with wrangling spatial data, Google Earth Engine provides a platform to further expand the potential fields of application for spatial data analysis."
  },
  {
    "objectID": "05_gge.html#reflections",
    "href": "05_gge.html#reflections",
    "title": "5  Google Earth Engine",
    "section": "5.3 Reflections",
    "text": "5.3 Reflections\nI feel this is a completely different take by Google toward geospatial data; Google Maps is obviously not the only contribution by this tech giant company in the field of GIScience. (I recall a speech from someone at the Google Maps team emphasising the idea of self-healing maps being supported by a huge number of end-users, but this is more developer-oriented.)\nAfter a few weeks of using massive earth observation data under the local environment, I have felt the limits of doing these analyses using limited computational power. Google Earth Engine lets us use tremendous amounts of data and cloud computers for analysis we could not have done before. The interesting addition to this is that you can publish apps that allow direct interaction with end-users!\nI hope to explore further the world of remote sensing data with this recently growing tool!\n\n\n\n\nAmani, Meisam, Arsalan Ghorbanian, Seyed Ali Ahmadi, Mohammad Kakooei, Armin Moghimi, S. Mohammad Mirmazloumi, Sayyed Hamed Alizadeh Moghaddam, et al. 2020. “Google Earth Engine Cloud Computing Platform for Remote Sensing Big Data Applications: A Comprehensive Review.” IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing 13: 5326–50. https://doi.org/10.1109/JSTARS.2020.3021052.\n\n\nBuonfantino, Giusy. 2022. “SC Johnson Forecasts Mosquitoes with Google Earth Engine.” Google Cloud Blog. https://cloud.google.com/blog/products/data-analytics/sc-johnson-forecasts-mosquitoes-with-google-earth-engine.\n\n\nGorelick, Noel, Matt Hancher, Mike Dixon, Simon Ilyushchenko, David Thau, and Rebecca Moore. 2017. “Google Earth Engine: Planetary-scale Geospatial Analysis for Everyone.” Remote Sensing of Environment, Big Remotely Sensed Data: Tools, applications and experiences, 202 (December): 18–27. https://doi.org/10.1016/j.rse.2017.06.031.\n\n\nSidhu, Nanki, Edzer Pebesma, and Gilberto Câmara. 2018. “Using Google Earth Engine to Detect Land Cover Change: Singapore as a Use Case.” European Journal of Remote Sensing 51 (1): 486–500. https://doi.org/10.1080/22797254.2018.1451782.\n\n\nTamiminia, Haifa, Bahram Salehi, Masoud Mahdianpari, Lindi Quackenbush, Sarina Adeli, and Brian Brisco. 2020. “Google Earth Engine for Geo-Big Data Applications: A Meta-Analysis and Systematic Review.” ISPRS Journal of Photogrammetry and Remote Sensing 164 (June): 152–70. https://doi.org/10.1016/j.isprsjprs.2020.04.001."
  },
  {
    "objectID": "06_classification.html#summary",
    "href": "06_classification.html#summary",
    "title": "6  Classification",
    "section": "6.1 Summary",
    "text": "6.1 Summary\n\n6.1.1 Overview\nClassification in the remote sensing context is to identify the different land uses based on the observations. This is often not the final output of research, but is incorporated as an important part of the data processing pipeline in many use cases. Identification of urban areas or vegetation are some common goals of classification. The main idea behind classification is to slice data in different ways, trying to differentiate the different characteristics of each usage.\n\n\n6.1.2 Methodology\nThere are many methodologies that achieve the clustering of earth observation data. 2 of the methodologies will be summarised below. The ones I could not cover include generic machine learning methodologies such as supervised / unsupervised machine learning techniques.\n\n6.1.2.1 Classification and Regression Trees (CART)\nThis methodology comprises of two parts: the classification tree that handles discrete data and regression trees that deal with continuous data.\nClassification trees take multiple rows of data as independent variables that contribute to the outcome decision. The more criteria the decision tree has, the smaller and purer each of the categories (leaves) become, but classifying into too small groups does not constitute an useful measure. Setting the point to stop may be defined by the minimum number of observations per leaf, or pruning leaves by considering tree scores.\nRegression trees are used for continuous values that can be classified into groups of observations each (seem to be) following a different pattern. The separating threshold is determined where the sum of the squared residuals become lowest - indicating the divide is significant.\nUsing only one tree may result in overfitting, thus not generalisable to new data. In order to overcome this issue, the random forest methodology using many different models (a collection of many trees = forest) is implemented to even out the impact.\n\n\n6.1.2.2 Support Vector Machine\nThe support vector machine (SVM) methodology is a linear binary classifier with soft margins. It finds the hyperplane within the high-dimensional space where it can divide groups with the largest margin between the support vectors (observations on other sides closest to divide) while minimizing misclassification.\nHyperparameters that impact the performance include \\(c\\) and \\(\\gamma\\) that corresponds to the slope and distance of control points, respectedly.\nThere will be differences in behaviour that can be observed. A one-to-one model tries to identify borders between each labeled group, while a one-to-rest approach tries to draw a border between the group of interest to the rest of the observations."
  },
  {
    "objectID": "06_classification.html#applications",
    "href": "06_classification.html#applications",
    "title": "6  Classification",
    "section": "6.2 Applications",
    "text": "6.2 Applications\nMany projects integrate classification of land cover as part of their projects. Sheykhmousa et al. (2020) conducted a research on the accuracy of the 2 major models discussed above: the SVM and the random forest approach for classification models. They have shown these models are superior in different scenarios, for instance the random forest approach performs better overall, but SVM outperforms random forest models in low-resolution imageries and observations including multiple features.\nKussul et al. (2017) is an example of an attempt to identify the different crop types using remote sensing data in Ukraine. The crops were identified using supervised classification into 7 different groups + forest, grassland, bare land and water. The overall accuracy of over 90% was recorded in most of the models, but has failed to consider the spatial autocorrelation of train and test models. The paper concludes we may be able to apply this to a wider region for a more systematic analysis.\nAn example of a research with classification as part of the data processing pipeline is conducted by Bhatia, Patil, and Buddhiraju (2023), where they investigate the urban sprawl of the Mumbai metropolitan area. They have used satellite imagery of the area to identify urban areas using supervised classification techniques. They have observed how the urban area of this city has grown over the past few decades, and have identified areas which have seen the most urban sprawl, thus identifying areas that require a stronger intervention on the policy level.\nResearch regarding classification can be categorised into 2 streams: trying to improve the performance of classification (such as the former example) and papers that address regional problems using the classification of urban areas. Although not explicitly titled in many research, classification is an underlying process in the many applications. One thing to keep in mind, however, is the measurement of accuracy. The latter has used the overall accuracy and the kappa value for identification, both of which have potential issues if not interpreted correctly. The train-test split must be done on a regional scale rather than the pixel level, where spatial autocorrelation may impact the outcome. The kappa value is inconsistent with a range of values available for the same level of confidence, thus making it not ideal for measuring performance."
  },
  {
    "objectID": "06_classification.html#reflections",
    "href": "06_classification.html#reflections",
    "title": "6  Classification",
    "section": "6.3 Reflections",
    "text": "6.3 Reflections\nIdentifying different land use from remote sensing is without a doubt an important topic, and I have got the chance to learn what is going on behind the scenes. I have found the idea of the regression tree to be interesting, being able to quantify gaps which could not be quantified using the original value. In our project, we will try to utilise this algorithm to measure the urban growth and extrapolate into the future, possibly getting insights for the necessary interventions that must be made in the Da Nang area in central Viet Nam.\n\n\n\n\nBhatia, S. Y., G. R. Patil, and K. M. Buddhiraju. 2023. “Analysing Urban Sprawl of the Mumbai Metropolitan Region Using Remote Sensing and Socioeconomic Data.” In The International Archives of Photogrammetry, Remote Sensing and Spatial Information Sciences, XLVIII-M-3-2023:35–42. Gottingen, Germany: Copernicus GmbH. https://doi.org/10.5194/isprs-archives-XLVIII-M-3-2023-35-2023.\n\n\nKussul, Nataliia, Mykola Lavreniuk, Sergii Skakun, and Andrii Shelestov. 2017. “Deep Learning Classification of Land Cover and Crop Types Using Remote Sensing Data.” IEEE Geoscience and Remote Sensing Letters 14 (5): 778–82. https://doi.org/10.1109/LGRS.2017.2681128.\n\n\nSheykhmousa, Mohammadreza, Masoud Mahdianpari, Hamid Ghanbari, Fariba Mohammadimanesh, Pedram Ghamisi, and Saeid Homayouni. 2020. “Support Vector Machine Versus Random Forest for Remote Sensing Image Classification: A Meta-Analysis and Systematic Review.” IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing 13: 6308–25. https://doi.org/10.1109/JSTARS.2020.3026724."
  },
  {
    "objectID": "01_intro.html#references",
    "href": "01_intro.html#references",
    "title": "1  Getting Started with Remote Sensing",
    "section": "1.4 References",
    "text": "1.4 References\n\n\n\n\nGillespie, Thomas W., Jasmine Chu, Elizabeth Frankenberg, and Duncan Thomas. 2007. “Assessment and Prediction of Natural Hazards from Satellite Imagery.” Progress in Physical Geography: Earth and Environment 31 (5): 459–70. https://doi.org/10.1177/0309133307083296.\n\n\nGISGeography. 2017. “The Atmospheric Window in Remote Sensing.” GIS Geography. https://gisgeography.com/atmospheric-window/.\n\n\nMapBiomas. n.d. “MapBiomas Alerta.” https://alerta.mapbiomas.org/en. Accessed February 7, 2024.\n\n\nNASA. 2019. “What Is Remote Sensing?  Earthdata.” Backgrounder. https://www.earthdata.nasa.gov/learn/backgrounders/remote-sensing; Earth Science Data Systems, NASA.\n\n\nTeixeira, Fabio. 2022. “In Brazil, Satellites Help Scientists Target Amazon Forest Losses.” Thomas Reuters Foundation News. https://news.trust.org/item/20220406135051-ut6cr/.\n\n\nYerushalmy, Jonathan. 2023. “Gaza Before and After: Satellite Images Show Destruction Following Israeli Airstrikes.” The Guardian, October."
  },
  {
    "objectID": "07_accuracy.html#summary",
    "href": "07_accuracy.html#summary",
    "title": "7  Accuracy and Sub- / Super-pixel analysis",
    "section": "7.1 Summary",
    "text": "7.1 Summary\nThis week we focus on measuring the accuracy of the classification, and object-based and sub-pixel analysis.\n\n7.1.1 Accuracy\n\n7.1.1.1 Accuracy Measures\nThe accuracy measures for remote sensing (and machine learning in general) is based upon comparing the positive / negative classifications to what the values actually were. The four possibilities that are considered can be organised as the following table.\n\nClassification accuracies and errors\n\n\n\n\n\n\n\n\n\n\nActually Positive\nActually Negative\nAccuracy Measure\nError\n\n\n\n\nPositively Classified\nTrue Positive\nFalse Positive\nUser’s Accuracy (Precision)\nCommission Error\n\n\nNegatively Classified\nFalse Negative\nTrue Negative\nNegative Predictive Value\nFalse Ommision Rate\n\n\nAccuracy Measure\nProducer’s Accuracy (Recall)\nTrue Negative Rate\n\n\n\n\nError\nOmission Error\nFalse Positive Rate\n\n\n\n\n\nThe producer’s accuracy is the rate of pixels correctly positively identified to the ground truth data. This shows the accuracy from the producer’s point of view, calculating the correctly classified data.\n\\[\n\\text{Producer's Accuracy} = \\frac{\\text{TP}}{\\text{TP}+\\text{FN}}\n\\]\nOn the other hand, the user’s accuracy defines the ratio of correctly identified pixels within the values that are classified as positive. This is the accuracy from the user’s perspective, as the classified data is taken as the dividend.\n\\[\n\\text{User's Accuracy} = \\frac{\\text{TP}}{\\text{TP}+\\text{FP}}\n\\]\nThe overall accuracy is the rate of correctly identified (positively or negatively) cells.\n\\[\n\\text{Overall Accuracy} = \\frac{\\text{TP}+\\text{TN}}{\\text{TP}+\\text{FP}+\\text{TN}+\\text{FN}}\n\\]\nFrom this overall accuracy measures, a widely used but inconsistent coefficient, the kappa coefficient, can be calculated. This values compares the accuracy to a randomly classified value. When \\(p_o\\) is the overall accuracy and \\(p_e\\) the expected accuracy when classified randomly, the kappa coefficient \\(\\kappa\\) can be calculated as:\n\\[\n\\kappa = \\frac{p_o-p_e}{1-p_e}\n\\]\nThe inconsistency in this coefficient is argued by Foody (2020), because there is a range of values that can be taken by the kappa coefficient, and it is difficult to interpret these values.\n\n\n7.1.1.2 Trade-offs\nThe producer’s accuracy (precision) and the user’s accuracy (recall) is a trade-off: if we wanted to improve the producer’s accuracy, we will lower the threshold to increase the number of positively identified pixels. This will lead to more false positive values as well, thus causing a decrease in the user’s accuracy. Therefore, there is a need to consider both of these values to determine a good balance between precision and recall. The F-1 score is one of these measures, combining these two factors (Wilber 2022a).\n\\[\nF_1 = \\frac{2 \\cdot \\text{PA} \\cdot \\text{UA}}{\\text{PA} + \\text{UA}}\n= \\frac{{\\text{TP}}}{\\text{TP} + \\frac{1}{2} \\left( \\text{FP} + \\text{FN} \\right)}\n\\]\nAnother measure is the area under the ROC (Receiver Operating Characteristics) curve. The ROC curve is an indicator of the classification characteristics with false positive rates on the x axis and true positive rates on the y axis.\n\n\n\nROC curve, excerpt from lecture slide. Originally from Wilber (2022b)\n\n\nThe area under this curve (AUC) can be a measurement of the classification method, where a larger area indicates a better classification technique. A random classification will have an AUC of 0.5, and a perfect classification will have a value of 1.\n\n\n7.1.1.3 Testing the accuracy\nAs with other machine learning, the training data and testing data must be different datasets. The following is a list of common procedures to generate training and testing data.\n\nTrain-test split: leaving a certain percentage of the dataset for testing, and not use for training\nCross-validation: split the dataset into \\(k\\) parts, leave one segment at a time to test with model trained with the remainder\nLeave one out cross-validation: an extreme example of the above, just leaving one observation at a time\n\nSpatial autocorrelation is an important factor that must be considered in this process, but the above mentioned techniques all do not consider this when applied naively. Having the testing data very close to the training data is an inappropriate setting, as it is very likely to have the similar characteristics as the training dataset. A spatial cross-validation considers the spatial distribution of the training and testing datasets, taking a spatial subset of the dataset to keep for testing.\n\n\n\nSpatial cross-validation, taking one area to leave behind for testing. Excerpt from Lovelace, Nowosad, and Muenchow (2019)\n\n\n\n\n\n7.1.2 Object-based analysis\nInstead of considering a single pixel at a time, the idea of object-based analysis is to take a group of cells (superpixels) clustered by their similarity or difference. A common way to achieve this is the Simple Linear Iterative Clustering method, where each cell is classified to the nearest centroid, with the distance considering both spectral and spatial distance. One simple example of this is:\n\\[\nd = \\sqrt{\\left( \\frac{d_c}{m} \\right)^2 + \\left( \\frac{d_s}{S} \\right)^2}\n\\]\nwhere - \\(d_c\\) spectral distance - \\(m\\) compactness parameter - \\(d_s\\) spatial distance - \\(S\\) interval between initial clusters\nImproved models for considering other distance measures (other than Euclidian) are created and implemented under the Supercells package.\nThe centroid is recalculated after the reclassification, and the process is iterated over multiple times (usually 5 - 10) to achieve the final output of classifications.\n\n\n\nExample of the SLIC method, excerpt from Nowosad and Stepinski (2021)\n\n\nClassification algorithms are overlaid for each of the clusters rather than focusing on individual clusters.\n\n\n7.1.3 Sub-pixel analysis\nIn contrast, sub-pixel analysis tries to identify the mixture within an area represented by a single pixel.\nSub-pixel analysis assumes the data a pixel obtains is a combination of multiple pure land covers - endmembers - mixed in a certain ratio, as illustrated below.\n\n\n\nIllustration of sub-pixel analysis, excerpt from Plaza et al. (2002)\n\n\nWhen the pixel values for each of the endmembers are observed as \\(p_{i\\lambda}\\), the mixed spectrum \\(p_{\\lambda}\\)can be broken down as\n\\[\np_{\\lambda} = \\sum_i{\\left( p_{i\\lambda}\\cdot f_i \\right)}+e_\\lambda\n\\] where \\(f_i\\) is the fractional cover for each of the endmembers, and \\(e_{\\lambda}\\) being the model error.\nA common model used in the urban context is the VIS model, where the endmembers Vegetation, Impervious surface, and Soil are considered.\nUnder this model, when the pixel values for each of the \\(k\\) bands are expressed as\n\\[\np =\n\\begin{bmatrix}\np_1 \\\\\np_2 \\\\\n\\vdots \\\\\np_k\n\\end{bmatrix}\n\\]\nthe fractional cover for each endmember can be calculated as shown below.\n\\[\n\\begin{gather}\n\\begin{cases}\np_{\\lambda} =\n\\begin{bmatrix}\np_V & p_I & p_S\n\\end{bmatrix}\n\\begin{bmatrix}\nf_V \\\\\nf_I \\\\\nf_S\n\\end{bmatrix}\n\\\\\nf_V + f_I + f_S = 1\n\\end{cases}\n\\\\ \\\\\n\\therefore\n\\begin{bmatrix}\np_{\\lambda 1} \\\\\np_{\\lambda 2} \\\\\n\\vdots \\\\\np_{\\lambda k} \\\\\n1\n\\end{bmatrix}\n=\n\\begin{bmatrix}\np_{V1} & p_{I1} & p_{S1} \\\\\np_{V2} & p_{I2} & p_{S2} \\\\\n\\vdots & \\vdots & \\vdots \\\\\np_{Vk} & p_{Ik} & p_{Sk} \\\\\n1 & 1 & 1\n\\end{bmatrix}\n\\begin{bmatrix}\nf_V \\\\\nf_I \\\\\nf_S\n\\end{bmatrix}\n\\\\\n\\Leftrightarrow\n\\begin{bmatrix}\nf_V \\\\\nf_I \\\\\nf_S\n\\end{bmatrix}\n=\n\\begin{bmatrix}\np_{V1} & p_{I1} & p_{S1} \\\\\np_{V2} & p_{I2} & p_{S2} \\\\\n\\vdots & \\vdots & \\vdots \\\\\np_{Vk} & p_{Ik} & p_{Sk} \\\\\n1 & 1 & 1\n\\end{bmatrix}^{-1}\n\\begin{bmatrix}\np_{\\lambda 1} \\\\\np_{\\lambda 2} \\\\\n\\vdots \\\\\np_{\\lambda k} \\\\\n1\n\\end{bmatrix}\n\\end{gather}\n\\]\nA limitation of this method can lie in identifying the endmembers, where it requires the existence of a pixel wit a ‘pure’ land cover - which may not be obvious."
  },
  {
    "objectID": "07_accuracy.html#applications",
    "href": "07_accuracy.html#applications",
    "title": "7  Accuracy and Sub- / Super-pixel analysis",
    "section": "7.2 Applications",
    "text": "7.2 Applications\nApplications of accuracy measures are often used for comparisons between models. Rozenstein and Karnieli (2011) explores the use of multiple classification models to identify the land cover to create a database of land cover for Israel. They have also conducted a post-classification accuracy improvement procedure to enhance their results of classification. The confusion matrix, producer’s and user’s accuracies, and the kappa coefficient was used to anlayse the accuracy of each method, concluding the hybrid approach of unsupervised and supervised learning methods have achieved the highest accuracy. One interesting point made is that the desired accuracy measure of 85% introduced in past research is not always achieved when informing planning decisions, including the outcomes of this research.\nRoberts et al. (2017) explores the need for cross-validation techniques that take into consideration not only spatial autocorrelation but also having other underlying structures within the dataset. Creating blocks in the spatial context, temporal blocking, and leaving out individual observations are considered as potential methodologies to prevent overfitting and underestimating the errors of the machine learning model. This study does point out, however, that extrapolation errors emerge as new problems and this requires consideration as well - in the spatial context, the shape of each ‘block’ is suggested to impact this error.\nThese research papers show the two aspects of accuracy in literature - the first paper is an example of accuracy used as part of the data pipeline that is helping the assessment of multiple models to serve a purpose, and the second model is the exploration of methodology that addresses and assesses accuracy. The first model uses the kappa coefficient - which is accused of being inconsistent but still being widely used - is an example of how this area is still under development and improvements must be made. The second paper shows that spatial autocorrelation is one of the many aspects that must be considered, which is an important thing to keep in mind."
  },
  {
    "objectID": "07_accuracy.html#reflections",
    "href": "07_accuracy.html#reflections",
    "title": "7  Accuracy and Sub- / Super-pixel analysis",
    "section": "7.3 Reflections",
    "text": "7.3 Reflections\nThis week we have observed the assessment of accuracy in remote sensing and procedures to improve and correctly handle this field.\nImportant messages to take home are:\n\njust because one measurement is widely used does not mean it is an appropriate one - we must consider whether the coefficients that we have are actually valid\nspatial autocorrelation must be considered when conducting classification - this is a new perspective that is unique when we are to handle spatial data in particular\n\nEven though this part does is not always our main interest, it is an important phase within our data processing pipeline that seeks to achieve other goals. Since accuracy is an inevitable aspect of academic research, we must keep in mind how to analyse the data correctly and with precision.\n\n\n\n\nFoody, Giles M. 2020. “Explaining the Unsuitability of the Kappa Coefficient in the Assessment and Comparison of the Accuracy of Thematic Maps Obtained by Image Classification.” Remote Sensing of Environment 239 (March): 111630. https://doi.org/10.1016/j.rse.2019.111630.\n\n\nLovelace, Robin, Jakub Nowosad, and Jannes Muenchow. 2019. Chapter 12 Statistical Learning  Geocomputation with R.\n\n\nNowosad, Jakub, and Tomasz Stepinski. 2021. “Generalizing the Simple Linear Iterative Clustering (SLIC) Superpixels.” https://nowosad.github.io/giscience-2021/.\n\n\nPlaza, A., P. Martinez, R. Perez, and J. Plaza. 2002. “Spatial/Spectral Endmember Extraction by Multidimensional Morphological Operations.” IEEE Transactions on Geoscience and Remote Sensing 40 (9): 2025–41. https://doi.org/10.1109/TGRS.2002.802494.\n\n\nRoberts, David R., Volker Bahn, Simone Ciuti, Mark S. Boyce, Jane Elith, Gurutzeta Guillera-Arroita, Severin Hauenstein, et al. 2017. “Cross-Validation Strategies for Data with Temporal, Spatial, Hierarchical, or Phylogenetic Structure.” Ecography 40 (8): 913–29. https://doi.org/10.1111/ecog.02881.\n\n\nRozenstein, Offer, and Arnon Karnieli. 2011. “Comparison of Methods for Land-Use Classification Incorporating Remote Sensing and GIS Inputs.” Applied Geography 31 (2): 533–44. https://doi.org/10.1016/j.apgeog.2010.11.006.\n\n\nWilber, Jared. 2022a. “Precision and Recall.” MLU-Explain. https://mlu-explain.github.io/precision-recall/.\n\n\n———. 2022b. “ROC and AUC.” MLU-Explain. https://mlu-explain.github.io/roc-auc/."
  },
  {
    "objectID": "08_sar.html#summary",
    "href": "08_sar.html#summary",
    "title": "8  Synthetic Aperture Radar (SAR)",
    "section": "8.1 Summary",
    "text": "8.1 Summary\n\n8.1.1 Overview\n\n\n8.1.2 Polarisation\nPolarisation is the orientation of the electromagnetic fields in relation to the direction of the wave - this can be classified as either horizontal or vertical.\n\nVideo illustration of electromagnetic waves. Video sourced from NASA’s Scientific Visualisation Studio (2017)\nA SAR satellite can transmit and receive horizontal and vertical waves. The four combinations of transmission and reception possible as follows:\n\nVV: Vertical transmission, Vertical reception\nVH: Vertical transmission, Horizontal reception\nHV: Horizontal transmission, Vertical reception\nHH: Horizontal transmission, Horizontal reception\n\nHV and VH - those with different orientation for transmission and reception are called cross-polarised reflectance.\nDifferent land covers reflect them in different behaviours - this enables us to identify the land covers.\n\nScattering strengths \\(|S|\\) of different land covers\n\n\n\n\n\n\n\nType\nExplanation\nScattering Strength\n\n\n\n\nRough surface scattering\nReflection from bare soil or water - most sensitive to VV\n\\(|S_{VV}| &gt; |S_{HH}| &gt; |S_{VH}|, |S_{HV}|\\)\n\n\nDouble bounce scattering\nReflection caused by buildings or standing structures, bouncing twice on the ground and an upright surface - most sensitive to HH\n\\(|S_{HH}| &gt; |S_{VV}| &gt; |S_{VH}|, |S_{HV}|\\)\n\n\nVolume scattering\nReflections from leaves or forest canopy - most sensitive to VH or HV\n\\(|S_{VH}|, |S_{HV}| &gt; |S_{VV}|, |S_{HH}|\\)\n\n\n\nThe different types of reflections are illustrated below.\n\n\n\nDifferent types of scattering occuring at different land covers (NASA Earth Science Data Systems 2020)\n\n\nOne extra point to note is that the wavelength also affects the reflection. A shorter wavelengths will be easily scattered by top layers of surface cover, while the longer wavelengths are capable to penetrate through. If observing forest canopy, longer bands might penetrate the leaves onto the ground, leading in more double bounce scattering than shorter wavelengths returning volume scattering.\n\n\n8.1.3 Interferometry"
  },
  {
    "objectID": "08_sar.html#applications",
    "href": "08_sar.html#applications",
    "title": "8  Synthetic Aperture Radar (SAR)",
    "section": "8.2 Applications",
    "text": "8.2 Applications"
  },
  {
    "objectID": "08_sar.html#reflection",
    "href": "08_sar.html#reflection",
    "title": "8  Synthetic Aperture Radar (SAR)",
    "section": "8.3 Reflection",
    "text": "8.3 Reflection\n\n\n\n\nNASA Earth Science Data Systems. 2020. “What Is Synthetic Aperture Radar?  Earthdata.” Backgrounder. https://www.earthdata.nasa.gov/learn/backgrounders/what-is-sar; Earth Science Data Systems, NASA.\n\n\nNASA’s Scientific Visualisation Studio. 2017. “NASA Scientific Visualization Studio  Electromagnetic Waves and Polarization.” SVS. https://svs.gsfc.nasa.gov/4580."
  }
]